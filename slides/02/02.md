title: NPFL139, Lecture 2
class: title, langtech, cc-by-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Value and Policy Iteration,<br>Monte Carlo, Temporal Difference

## Milan Straka

### February 26, 2025

---
section: Refresh
class: section
# Refresh

---
# Markov Decision Process

![w=85%,h=center,v=middle](../01/mdp.svgz)

~~~~
# Markov Decision Process

![w=47%,h=center](../01/mdp.svgz)

A **Markov decision process** (MDP) is a quadruple $(𝓢, 𝓐, p, γ)$,
where:
- $𝓢$ is a set of states,
~~~
- $𝓐$ is a set of actions,
~~~
- $p(S_{t+1} = s', R_{t+1} = r | S_t = s, A_t = a)$ is a probability that
  action $a ∈ 𝓐$ will lead from state $s ∈ 𝓢$ to $s' ∈ 𝓢$, producing a **reward** $r ∈ ℝ$,
~~~
- $γ ∈ [0, 1]$ is a **discount factor**.

~~~
Let a **return** $G_t$ be $G_t ≝ ∑_{k=0}^∞ γ^k R_{t + 1 + k}$. The goal is to optimize $𝔼[G_0]$.

---
# Partially Observable MDPs

![w=68%,h=center](../01/pomdp.svgz)

**Partially observable Markov decision process** extends the Markov decision
process to a sextuple $(𝓢, 𝓐, p, γ, 𝓞, o)$, where in addition to an MDP,
- $𝓞$ is a set of observations,
- $o(O_{t+1} | S_{t+1}, A_t)$ is an observation model, where observation $O_t$ is used as agent input
  instead of the state $S_t$.

---
# Episodic and Continuing Tasks

If the agent-environment interaction naturally breaks into independent
subsequences, usually called **episodes**, we talk about **episodic tasks**.
Each episode then ends in a special **terminal state**, followed by a reset
to a starting state (either always the same, or sampled from a distribution
of starting states).

~~~
In episodic tasks, it is often the case that every episode ends in at most
$H$ steps. These **finite-horizon tasks** then can use discount factor $γ=1$,
because the return $G ≝ ∑_{t=0}^H γ^t R_{t + 1}$ is well defined.

~~~
If the agent-environment interaction goes on and on without a limit, we instead
talk about **continuing tasks**. In this case, the discount factor $γ$ needs
to be sharply smaller than 1.

---
# (State-)Value and Action-Value Functions

A **policy** $π$ computes a distribution of actions in a given state, i.e.,
$π(a | s)$ corresponds to a probability of performing an action $a$ in state
$s$.

~~~
To evaluate a quality of a policy, we define **value function** $v_π(s)$, or
**state-value function**, as
$$\begin{aligned}
  v_π(s) & ≝ 𝔼_π\big[G_t \big| S_t = s\big] = 𝔼_π\left[∑\nolimits_{k=0}^∞ γ^k R_{t+k+1} \middle| S_t=s\right] \\
         & = 𝔼_{A_t ∼ π(s)} 𝔼_{S_{t+1},R_{t+1} ∼ p(s,A_t)} \big[R_{t+1}
           + γ 𝔼_{A_{t+1} ∼ π(S_{t+1})} 𝔼_{S_{t+2},R_{t+2} ∼ p(S_{t+1},A_{t+1})} \big[R_{t+2} + … \big]\big]
\end{aligned}$$

~~~
An **action-value function** for a policy $π$ is defined analogously as
$$q_π(s, a) ≝ 𝔼_π\big[G_t \big| S_t = s, A_t = a\big] = 𝔼_π\left[∑\nolimits_{k=0}^∞ γ^k R_{t+k+1} \middle| S_t=s, A_t = a\right].$$

~~~
The value function and action-value function can be of course expressed using one another:
$$v_π(s) = 𝔼_{a∼π}\big[q_π(s, a)\big],~~~~~~~q_π(s, a) = 𝔼_{s', r ∼ p}\big[r + γv_π(s')\big].$$

---
# Optimal Value Functions

Optimal state-value function is defined as
$$v_*(s) ≝ \max_π v_π(s),$$
~~~
analogously
$$q_*(s, a) ≝ \max_π q_π(s, a).$$

~~~
Any policy $π_*$ with $v_{π_*} = v_*$ is called an **optimal policy**. Such policy
can be defined as $π_*(s) ≝ \argmax_a q_*(s, a) = \argmax_a 𝔼\big[R_{t+1} + γv_*(S_{t+1}) | S_t = s, A_t = a\big]$.
When multiple actions maximize $q_*(s, a)$, the optimal policy can
stochastically choose any of them.

~~~
## Existence
In finite-horizon tasks or if $γ < 1$, there always exists a unique optimal
state-value function, a unique optimal action-value function, and a (not necessarily
unique) optimal policy.

---
section: DP
class: section
# Dynamic Programming

---
# Dynamic Programming

Dynamic programming is an approach devised by Richard Bellman in 1950s.

~~~
To apply it to MDP, we now consider finite-horizon problems with finite number
of states $𝓢$, finite number of actions $𝓐$, and known MDP dynamics $p$.
Note that without loss of generality, we can assume that every episode takes
exactly $H$ steps (by introducing a suitable absorbing state, if necessary).

~~~
The following recursion is usually called
the **Bellman equation**:
$$\begin{aligned}
  v_*(s) &= \max_a 𝔼\big[R_{t+1} + γ v_*(S_{t+1}) \big| S_t=s, A_t=a\big] \\
         &= \max_a ∑_{s', r} p(s', r | s, a) \big[r + γ v_*(s')\big];~~\textrm{~0 if~}s\textrm{~is terminal}.
\end{aligned}$$

~~~
It must hold for an optimal value function in a MDP, because future decisions
do not depend on the current one. Therefore, the optimal policy can be
expressed as one action followed by optimal policy from the resulting state.

---
# Dynamic Programming

To turn the Bellman equation into an algorithm, we change the equal sign to an assignment:
$$\begin{aligned}
v_0(s) &← \begin{cases} 0&\textrm{for the terminal state $s$,} \\ -∞&\textrm{otherwise;} \end{cases} \\
v_{k+1}(s) &← \max_a 𝔼\big[R_{t+1} + γ v_k(S_{t+1}) \big| S_t=s, A_t=a\big];~~\textrm{~0 if~}s\textrm{~is terminal}.
\end{aligned}$$

~~~
In a finite-horizon task with $H$ steps, the optimal value function is reached
after $H$ iterations of the above assignment:
~~~
- We can show by induction that $v_k(s)$ is the maximum return reachable from
  state $s$ in last $k$ steps of an episode.
~~~
- If every episode ends in at most $H$ steps, then $v_{H+1}$ must be equal
  to $v_H$.

---
# Relations to Graph Algorithms

In current settings, searching for the optimal value function of a deterministic
MDP problem (i.e., when there are always just a single next state and a single
reward) is in fact the same as searching for the longest (maximum weighted) path
in a suitable graph:

![w=60%,h=center](value_trellis.svgz)

where the value of an edge going from $v_{t+1}(s_i)$ to $v_t(s_j)$ is either the
highest reward some transition $p(s_i, a) → (s_j, r)$ produces, or $-∞$ if
no action from the state $s_i$ leads to $s_j$.

---
# Bellman-Ford-Moore Algorithm

Consider the dynamic programming algorithm of the repeated Bellman equation application:
$$\begin{aligned}
v_0(s) &← \begin{cases} 0&\textrm{for the terminal state $s$,} \\ -∞&\textrm{otherwise;} \end{cases} \\
v_{k+1}(s) &← \max_a 𝔼\big[R_{t+1} + γ v_k(S_{t+1}) \big| S_t=s, A_t=a\big];~~\textrm{~0 if~}s\textrm{~is terminal}.
\end{aligned}$$

~~~
The Bellman-Ford-Moore shortest-path algorithm can be considered its special-case:
```python
# input: graph `g`, initial vertex `s`
for v in g.vertices:
  d[v] = 0 if v == s else +∞

for iteration in range(len(g.vertices) - 1):
  for e in g.edges:
    if d[e.source] + e.length < d[e.target]:
      d[e.target] = d[e.source] + e.length

```
---
# Uniqueness of the Bellman Equation Solution

Not only does the optimal value function fulfill the Bellman equation in the
current settings, the converse is also true: If a value function satisfies
the Bellman equation and also assigns zero to the terminal state(s), it is optimal.
~~~

To sketch the proof of the statement, consider for a contradiction that
some solution of the Bellman equation is not an optimal value function.
Therefore, there exist states with different than optimal values.
~~~

Among those states, we choose such a state that all trajectories from it
contain only states with optimal values. We can find it by starting in an
arbitrary state with different than optimal value, and then repeatedly
transitioning into a reachable state with different than optimal value function.

For such a state, however, if its value is not optimal, then the Bellman
equation cannot hold in this state, which is a contradiction.

---
section: Value Iteration
class: section
# Value Iteration Algorithm

---
# Bellman Backup Operator

Our goal is now to handle also infinite-horizon tasks, using discount factor of
$γ < 1$. However, we still assume finite number of states and actions.

~~~
For any value function $v∈ℝ^{|𝓢|}$ we define **Bellman backup operator** $B : ℝ^{|𝓢|} \rightarrow ℝ^{|𝓢|}$ as
$$Bv(s) ≝ \max_a 𝔼\big[R_{t+1} + γ v(S_{t+1}) \big| S_t=s, A_t=a\big];~~\textrm{~0 if~}s\textrm{~is terminal}.$$

~~~
Considering the supremum norm $\|x\|_∞ ≝ \sup_s |x(s)|$, we will show that
Bellman backup operator is a _contraction_, i.e.,
$$\sup_s \big|Bv_1(s) - Bv_2(s)\big| = \big\|Bv_1 - Bv_2\big\|_∞ ≤ γ \|v_1 - v_2\|_∞.$$

~~~
(Note that the proof applies also to the case where there is an infinite amount
of states.)

~~~
Applying the Banach fixed-point theorem on the normed vector space $ℝ^{|𝓢|}$ with the supremum norm
then yields that there exists a _unique value function_ $v_*$ such that $Bv_* = v_*$.  
~~~
Such a unique $v_*$ is the _optimal value function_, because it satisfies the
Bellman equation.

---
# Bellman Backup Operator

Furthermore, iterative application of $B$ on arbitrary $v$ converges to $v_*$,
because
$$\big\|Bv - v_*\big\|_∞ = \big\|Bv - Bv_*\big\|_∞ ≤ γ\|v - v_*\|,$$
and therefore $B^nv \rightarrow v_*$.

---
# Value Iteration Algorithm

We can turn the iterative application of Bellman backup operator into an
algorithm.
$$Bv(s) ≝ \max_a 𝔼\big[R_{t+1} + γ v(S_{t+1}) \big| S_t=s, A_t=a\big];~~\textrm{~0 if~}s\textrm{~is terminal}.$$

![w=75%,h=center](value_iteration_synchronous.svgz)

---
# Value Iteration Algorithm

Although we have described the so-called _synchronous_ implementation requiring
two arrays for $v$ and $Bv$, usual implementations are _asynchronous_ and modify
the value function in place (if a fixed ordering is used, usually such value
iteration is called _Gauss-Seidel_).

![w=32%,f=right](value_iteration_asynchronous.svgz)

<div class="algorithm">

- for $s ∈ S$ in some fixed order:
  - $v(s) ← \max_a 𝔼\big[R_{t+1} + γ v(S_{t+1}) \big| S_t=s, A_t=a\big]$
</div>

~~~
Even with such asynchronous update, value iteration can be proven to converge,
and usually performs better in practice.

~~~
For example, the Bellman-Ford-Moore algorithm also updates the distances
in-place. In the case of dynamic programming, we can extend the invariant
from “$v_k(s)$ is the maximum return reachable from state $s$ in last $k$ steps
of an episode” to include not only all trajectories of $k$ steps, but possibly
also some number of longer trajectories.

~~~
If you are interested, try proving that the above Gauss-Seidel iteration is also
a contraction.

---
# Bellman Backup Operator as a Contraction

To show that Bellman backup operator is a contraction, we proceed as follows:

~~~
$\displaystyle \big\|Bv_1 - Bv_2\big\|_∞ = \big\|\max_a 𝔼\big[R_{t+1} + γ v_1(S_{t+1})\big] - \max_a 𝔼\big[R_{t+1} + γ v_2(S_{t+1})\big]\big\|_∞$

~~~
$\displaystyle \phantom{\big\|Bv_1 - Bv_2\big\|_∞} ≤ \big\|\max_a \big(𝔼\big[R_{t+1} + γ v_1(S_{t+1})\big] - 𝔼\big[R_{t+1} + γ v_2(S_{t+1})\big]\big)\big\|_∞$

~~~
$\displaystyle \phantom{\big\|Bv_1 - Bv_2\big\|_∞} = \Big\|\max_a \Big(∑\nolimits_{s', r} p(s', r | s, a)\big(r + γv_1(s') - r - γv_2(s')\big)\Big)\Big\|_∞$

~~~
$\displaystyle \phantom{\big\|Bv_1 - Bv_2\big\|_∞} = \Big\|\max_a \Big(𝔼\big[γ\big(v_1(S_{t+1}) - v_2(S_{t+1})\big)\big]\Big)\Big\|_∞$

~~~
$\displaystyle \phantom{\big\|Bv_1 - Bv_2\big\|_∞} = γ \Big\|\max_a \Big(𝔼\big[v_1(S_{t+1}) - v_2(S_{t+1})\big]\Big)\Big\|_∞$

~~~
$\displaystyle \phantom{\big\|Bv_1 - Bv_2\big\|_∞} ≤ γ \Big\|\max_a \Big(𝔼\big[\big\|v_1 - v_2\big\|_∞\big]\Big)\Big\|_∞$

~~~
$\displaystyle \phantom{\big\|Bv_1 - Bv_2\big\|_∞} = γ \|v_1 - v_2\|_∞.$

---
# Speed of Convergence

Assuming maximum reward is $R_\textrm{max}$, we have that
$$v_*(s) ≤ ∑_{t=0}^∞ γ^t R_\textrm{max} = \frac{R_\textrm{max}}{1-γ}.$$

~~~
Starting with $v(s) ← 0$, we have
$$\big\|B^k v - v_*\big\|_∞ ≤ γ^k \|v - v_*\|_∞ ≤ γ^k \frac{R_\textrm{max}}{1-γ}.$$

~~~
Compare to finite-horizon case, where $B^T v = v_*$.

---
# Value Iteration Example

Consider a simple betting game, where a gambler repeatedly bets on the outcome
of a coin flip (with a given win probability), either losing their stake or
winning the same amount of coins that was bet. The gambler wins if they obtain
100 coins, and lose if they run our of money.

~~~
We can formulate the problem as an undiscounted episodic MDP. The states
are the coins owned by the gambler, $\{1, …, 99\}$, and actions are the
stakes $\{1, …, \min(s, 100-s)\}$. The reward is $+1$ when reaching 100
and 0 otherwise.

~~~
The state-value function then gives probability of winning from each state,
and policy prescribes a stake with a given capital.

---
# Value Iteration Example

For a coin flip win probability 40%, the value iteration proceeds as follows.

![w=91%,h=center](value_iteration_example.svgz)

---
section: Policy Iteration
class: section
# Policy Iteration Algorithm

---
# Policy Iteration Algorithm

We now propose another approach of computing optimal policy. The approach,
called **policy iteration**, consists of repeatedly performing policy
**evaluation** and policy **improvement**.

## Policy Evaluation

Given a policy $π$, policy evaluation computes $v_π$.

Recall that
$$\begin{aligned}
  v_π(s) &≝ 𝔼_π\big[G_t \big| S_t = s\big] \\
         &= 𝔼_π\big[R_{t+1} + γ v_π(S_{t+1}) \big | S_t = s\big] \\
         &= ∑\nolimits_a π(a|s) ∑\nolimits_{s', r} p(s', r | s, a) \big[r + γ v_π(s')\big];~~\textrm{~0 if~}s\textrm{~is terminal}.
\end{aligned}$$

If the dynamics of the MDP $p$ is known, the above is a system of linear
equations, and therefore, $v_π$ can be computed exactly.

---
# Policy Evaluation
The equation
$$v_π(s) = ∑\nolimits_a π(a|s) ∑\nolimits_{s', r} p(s', r | s, a) \left[r + γ v_π(s')\right];~~\textrm{~0 if~}s\textrm{~is terminal}$$
is called **Bellman equation for $v_π$** and analogously to Bellman optimality
equation, it can be proven that
- under the same assumptions as before ($γ<1$ or termination), $v_π$ exists and is unique;
~~~
- the fixed point of the Bellman equation is exactly the value function $v_π$;
~~~
- the iterative application of the Bellman equation to any $v$ converges to $v_π$
  (the proof is simpler than for the optimality equation, because $v_π$ is
  defined using an expectation and expectations are linear, so we get the first half
  of the proof “for free”).

---
class: middle
# Policy Evaluation

![w=100%](policy_evaluation.svgz)

---
# Policy Improvement

Given $π$ and computed $v_π$, we would like to **improve** the policy.
A straightforward way to do so is to define a policy using a _greedy_ action
$$\begin{aligned}
  π'(s) &≝ \argmax_a q_π(s, a) \\
        &= \argmax_a ∑\nolimits_{s', r} p(s', r | s, a) \big[r + γ v_π(s')\big].
\end{aligned}$$

For such $π'$, by construction it obviously holds that
$$q_π(s, π'(s)) ≥ v_π(s).$$

---
# Policy Improvement Theorem

Let $π$ and $π'$ be any pair of deterministic policies, such that
$q_π(s, π'(s)) ≥ v_π(s)$.

Then for all states $s$, $v_{π'}(s) ≥ v_π(s)$.

~~~
The proof is straightforward, we repeatedly expand $q_π$ and use the
assumption of the policy improvement theorem:

~~~
$\displaystyle \qquad v_π(s) ≤ q_π(s, π'(s))$

~~~
$\displaystyle \qquad \phantom{v_π(s)} = 𝔼[R_{t+1} + γ v_π(S_{t+1}) | S_t = s, A_t = π'(s)]$

~~~
$\displaystyle \qquad \phantom{v_π(s)} = 𝔼_{π'}[R_{t+1} + γ v_π(S_{t+1}) | S_t = s]$

~~~
$\displaystyle \qquad \phantom{v_π(s)} ≤ 𝔼_{π'}[R_{t+1} + γ q_π(S_{t+1}, π'(S_{t+1})) | S_t = s]$

~~~
$\displaystyle \qquad \phantom{v_π(s)} = 𝔼_{π'}[R_{t+1} + γ 𝔼[R_{t+2} + γ v_π(S_{t+2}) | S_{t+1}, A_{t+1} = π'(S_{t+1})] | S_t = s]$

~~~
$\displaystyle \qquad \phantom{v_π(s)} = 𝔼_{π'}[R_{t+1} + γ R_{t+2} + γ^2 v_π(S_{t+2}) | S_t = s]$

~~~
$\displaystyle \qquad \phantom{v_π(s)} …$

~~~
$\displaystyle \qquad \phantom{v_π(s)} ≤ 𝔼_{π'}[R_{t+1} + γ R_{t+2} + γ^2 R_{t+3} + … | S_t = s] = v_{π'}(s)$

---
# Policy Improvement Example

![w=50%](gridworld_4x4.svgz)![w=60%,mw=50%,h=center](gridworld_4x4_policy_evaluation.svgz)

---
# Policy Iteration Algorithm

Policy iteration consists of repeatedly performing policy evaluation and policy
improvement:
$$π_0 \stackrel{E}{\longrightarrow} v_{π_0} \stackrel{I}{\longrightarrow}
  π_1 \stackrel{E}{\longrightarrow} v_{π_1} \stackrel{I}{\longrightarrow}
  π_2 \stackrel{E}{\longrightarrow} v_{π_2} \stackrel{I}{\longrightarrow}
  … \stackrel{I}{\longrightarrow} π_* \stackrel{E}{\longrightarrow} v_{π_*}.$$

~~~
The result is a sequence of monotonically improving policies $π_i$. Note that
when $π' = π$, also $v_{π'} = v_π$, which means Bellman optimality equation is
fulfilled and both $v_π$ and $π$ are optimal.

~~~
Considering that there is only a finite number of policies, the optimal policy
and optimal value function can be computed in finite time (contrary to value
iteration, where the convergence is only asymptotic).

~~~
Note that when evaluating policy $π_{k+1}$, we usually start with $v_{π_k}$,
which is assumed to be a good approximation to $v_{π_{k+1}}$.

---
# Policy Iteration Algorithm
![w=70%,h=center](policy_iteration.svgz)

---
# Value Iteration as Policy Iteration

Note that value iteration is in fact a policy iteration, where policy evaluation
is performed only for one step:

$$\begin{aligned}
  π'(s) &= \argmax_a ∑\nolimits_{s', r} p(s', r | s, a) \big[r + γ v(s')\big] &\textit{(policy improvement)} \\
  v'(s) &= ∑\nolimits_a π'(a|s) ∑\nolimits_{s', r} p(s', r | s, a) \big[r + γ v(s')\big] &\textit{(one step of policy evaluation)}
\end{aligned}$$

Substituting the former into the latter, we get
$$v'(s) = \max_a ∑\nolimits_{s', r} p(s', r | s, a) \big[r + γ v(s')\big] = Bv(s).$$

---
# Generalized Policy Iteration

Therefore, it seems that to achieve convergence, it is not necessary to perform
the policy evaluation exactly.

**Generalized Policy Evaluation** is a general concept of interleaving policy
evaluation and policy improvement at various granularity.

~~~
![w=30%,mw=50%,h=center](gpi.svgz)![w=80%,mw=50%,h=center](gpi_convergence.svgz)

If both processes stabilize, we know we have obtained optimal policy.

---
section: Monte Carlo
class: section
# Monte Carlo Methods

---
# Monte Carlo Methods

Monte Carlo methods are based on estimating returns from complete episodes.
Furthermore, if the model (of the environment) is not known, we need to
estimate returns for the action-value function $q$ instead of $v$.

~~~
We can formulate Monte Carlo methods in the generalized policy improvement
framework. Keeping estimated returns for the action-value function, we perform
policy evaluation by sampling one episode according to current policy. We then
update the action-value function by averaging over the observed returns,
including the currently sampled episode.

---
# Monte Carlo Methods

To hope for convergence, we need to visit each state infinitely many times.
One of the simplest way to achieve that is to assume _exploring starts_, where
we randomly select the first state and first action, each pair with nonzero
probability.

~~~
Furthermore, if a state-action pair appears multiple times in one episode, the
sampled returns are not independent. Literature distinguishes two cases:
~~~
- **first visit**: only the first occurence of a state-action pair in an episode is
  considered (i.e., we update this pair only once using the return observed
  during the first visit)
~~~
- **every visit**: all occurences of a state-action pair are considered (i.e.,
  the pair is updated several times using all the returns observed from it).

~~~
Even though first-visit is easier to analyze, it can be proven that for both
approaches, policy evaluation converges (the law of large numbers suffices
for the first-visit case; the every-visit case requires special handling, see
Singh and Sutton: Reinforcement learning with replacing eligibility traces, 1996).

~~~
Contrary to the Reinforcement Learning: An Introduction book, which presents
first-visit algorithms, we use every-visit variants, because they are used in
practice.

---
# Monte Carlo with Exploring Starts

![w=90%,h=center](../01/monte_carlo_exploring_starts.svgz)

---
# Monte Carlo Prediction of Blackjack

![w=82%,h=center](../01/blackjack_estimation.svgz)

---
# Monte Carlo Optimal Policy on Blackjack

![w=81%,h=center](../01/blackjack_optimal.svgz)

---
# Monte Carlo and $ε$-soft Policies

The problem with exploring starts is that in many situations, we either cannot
start in an arbitrary state, or it is impractical.

~~~
A policy is called $ε$-soft, if
$$π(a|s) ≥ \frac{ε}{|𝓐(s)|}.$$
and we call it $ε$-greedy, if one action has a maximum probability of
$1-ε+\frac{ε}{|A(s)|}$.

~~~
The policy improvement theorem can be proved also for the class of $ε$-soft
policies, and using $ε$-greedy policy in policy improvement step, policy
iteration has the same convergence properties. (We can embed the $ε$-soft behaviour
“inside” the environment and prove equivalence.)

---
# Monte Carlo for $ε$-soft Policies

### On-policy every-visit Monte Carlo for $ε$-soft Policies
Algorithm parameter: small $ε>0$

Initialize $Q(s, a) ∈ ℝ$ arbitrarily (usually to 0), for all $s ∈ 𝓢, a ∈ 𝓐$<br>
Initialize $C(s, a) ∈ ℤ$ to 0, for all $s ∈ 𝓢, a ∈ 𝓐$

Repeat forever (for each episode):
- Generate an episode $S_0, A_0, R_1, …, S_{T-1}, A_{T-1}, R_T$,
  by generating actions as follows:
  - With probability $ε$, generate a random uniform action
  - Otherwise, set $A_t ≝ \argmax\nolimits_a Q(S_t, a)$
- $G ← 0$
- For each $t=T-1, T-2, …, 0$:
  - $G ← γG + R_{t+1}$
  - $C(S_t, A_t) ← C(S_t, A_t) + 1$
  - $Q(S_t, A_t) ← Q(S_t, A_t) + \frac{1}{C(S_t, A_t)}(G - Q(S_t, A_t))$

---
section: Afterstates
class: section
# Action-values and Afterstates

---
# Action-values and Afterstates

The reason we estimate _action-value_ function $q$ is that the policy is
defined as
$$\begin{aligned}
  π(s) &≝ \argmax_a q_π(s, a) \\
       &= \argmax_a ∑\nolimits_{s', r} p(s', r | s, a) \left[r + γ v_π(s')\right]
\end{aligned}$$
and the latter form might be impossible to evaluate if we do not have the model
of the environment.

~~~
![w=80%,mw=40%,h=center,f=right](afterstates.svgz)
However, if the environment is known, it is often better to estimate returns only
for states, because there can be substantially less states than state-action pairs.

---
section: TD
class: section
# Temporal-difference (TD) Methods

---
section: TD
# Temporal-difference (TD) Methods

Temporal-difference methods estimate action-value returns using one iteration of
Bellman equation instead of complete episode return.

~~~
Compared to Monte Carlo method with constant learning rate $α$, which performs
$$v(S_t) ← v(S_t) + α\big(G_t - v(S_t)\big),$$
the simplest temporal-difference method computes the following:
$$v(S_t) ← v(S_t) + α\big(R_{t+1} + [¬\textrm{done}]⋅γv(S_{t+1}) - v(S_t)\big),$$
where $[¬\textrm{done}]$ has a value of 1 if the episode continues in
the state $S_{t+1}$, and 0 otherwise.

~~~
We say TD methods are **bootstrapping**, because they base their update on an
existing (action-)value function estimate.

---
# Temporal-difference (TD) Methods

![w=70%,h=center](td_example.svgz)

~~~
![w=70%,h=center](td_example_update.svgz)

---
# Temporal-difference (TD) Methods

An obvious advantage of TD methods compared to Monte Carlo is that
they are naturally implemented in _online_, _fully incremental_ fashion,
while the Monte Carlo methods must wait until an episode ends, because only then
the return is known.

~~~
The possibility of immediate learning is useful for:
- continuous environments,

~~~
- environments with extremely large episodes,
~~~
- environments ending after some nontrivial goal is reached, requiring some
  coordinated strategy from the agent (i.e., it is improbable that random
  actions will reach it).

---
# TD and MC Comparison

As with Monte Carlo methods, for a fixed policy $π$ (i.e., the policy evaluation
part of the algorithms), TD methods converge to $v_π$.

~~~
On stochastic tasks, TD methods usually converge to $v_π$ faster than constant-$α$ MC
methods.

~~~
![w=70%,h=center](td_mc_comparison_example.svgz)

~~~
![w=70%,h=center](td_mc_comparison.svgz)

---
# Optimality of MC and TD Methods

![w=58%,mw=50%,h=center](td_mc_optimality_example.svgz)![w=90%,mw=50%,h=center](td_mc_optimality_data.svgz)

~~~
For state B, 6 out of 8 times return from B was 1 and 0 otherwise.
Therefore, $v(B) = 3/4$.

~~~
- [TD] For state A, in all cases it transferred to B. Therefore, $v(A)$ could be $3/4$.
~~~
- [MC] For state A, in all cases it generated return 0. Therefore, $v(A)$ could be $0$.

~~~
MC minimizes mean squared error on the returns from the training data, while TD
finds the estimates that would be exactly correct for a maximum-likelihood
estimate of the Markov process model (the estimated transition probability from
$s$ to $t$ is the fraction of observed transitions from $s$ that went to $t$, and
the corresponding reward is the average of the rewards observed on those
transitions).

---
section: Sarsa
class: section
# Sarsa Algorithm

---
# Sarsa Algorithm

A straightforward application to the temporal-difference policy evaluation
is Sarsa algorithm, which after generating $S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}$
computes
$$q(S_t, A_t) ← q(S_t, A_t) + α\big(R_{t+1} + [¬\textrm{done}]⋅γ q(S_{t+1}, A_{t+1}) -q(S_t, A_t)\big).$$

~~~
![w=75%,h=center](sarsa.svgz)

---
# Sarsa Algorithm

![w=65%,h=center](sarsa_example.svgz)

~~~
MC methods cannot be easily used, because an episode might not terminate if
the current policy causes the agent to stay in the same state.

---
section: Q-learning
class: section
# Q-learning Algorithm

---
section: Q-learning
# Q-learning Algorithm

Q-learning was an important early breakthrough in reinforcement learning (Watkins, 1989).

$$q(S_t, A_t) ← q(S_t, A_t) + α\Big(R_{t+1} +  [¬\textrm{done}]⋅γ \max_a q(S_{t+1}, a) -q(S_t, A_t)\Big).$$

~~~
![w=80%,h=center](q_learning.svgz)

---
# Q-learning versus Sarsa

![w=100%,h=center](cliff_walking.svgz)

~~~ ~
# Q-learning versus Sarsa
![w=43%,h=center](cliff_walking.svgz)
![w=45%,h=center](cliff_walking_learning.svgz)

---
class: summary
# Summary

- The optimal value and action-value functions exist and are unique for:
  - finite-horizon tasks with or without the discount factor $γ$,
  - arbitrary episodic tasks when $γ < 1$;
- Value iteration and policy iteration:
  - require a known MDP, finite states, and finite actions,
  - converge to the optimal value function and policy,
    - a variant of policy iteration converges in a finite number of steps;
- Monte Carlo methods:
  - do not require the knowlege of the MDP,
  - learn from whole episodes,
  - Monte Carlo prediction itself converges to the value function for a given
    policy;
- Temporal-difference methods:
  - do not require the knowlege of the MDP,
  - learn even during episodes;
