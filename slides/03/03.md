title: NPFL139, Lecture 3
class: title, langtech, cc-by-sa
# Off-Policy Methods, N-step, Function Approximation

## Milan Straka

### March 5, 2025

---
section: Refresh
class: section
# Refresh

---
# Sarsa

A straightforward application to the temporal-difference policy evaluation
is Sarsa algorithm, which after generating $S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}$
computes
$$q(S_t, A_t) ← q(S_t, A_t) + α\big(R_{t+1} + [¬\textrm{done}]⋅γ q(S_{t+1}, A_{t+1}) -q(S_t, A_t)\big).$$

~~~
![w=75%,h=center](../02/sarsa.svgz)

---
# Sarsa

![w=65%,h=center](../02/sarsa_example.svgz)

~~~
MC methods cannot be easily used, because an episode might not terminate if
the current policy causes the agent to stay in the same state.

---
# Q-learning

Q-learning was an important early breakthrough in reinforcement learning (Watkins, 1989).

$$q(S_t, A_t) ← q(S_t, A_t) + α\Big(R_{t+1} +  [¬\textrm{done}]⋅γ \max_a q(S_{t+1}, a) -q(S_t, A_t)\Big).$$

~~~
![w=80%,h=center](../02/q_learning.svgz)

---
# Q-learning versus Sarsa

![w=100%,h=center](../02/cliff_walking.svgz)

~~~ ~
# Q-learning versus Sarsa
![w=43%,h=center](../02/cliff_walking.svgz)
![w=45%,h=center](../02/cliff_walking_learning.svgz)

---
section: Off-policy Methods
class: section
# On-policy and Off-policy Methods

---
# On-policy and Off-policy Methods

So far, most methods were **on-policy**. The same policy was used both for
generating episodes and as a target of value function.

~~~
However, while the policy for generating episodes needs to be more exploratory,
the target policy should capture optimal behaviour.

~~~
Generally, we can consider two policies:
- **behaviour** policy, usually $b$, is used to generate behaviour and can be more
  exploratory;
~~~
- **target** policy, usually $π$, is the policy being learned (ideally the optimal
  one).

~~~
When the behaviour and target policies differ, we talk about **off-policy**
learning.

---
# On-policy and Off-policy Methods

The off-policy methods are usually more complicated and slower to converge, but
are able to process data generated by different policy than the target one.

~~~
The advantages of off-policy methods are:
- capability of producing the optimal non-stochastic (non-exploratory) policies;

~~~
- more exploratory behaviour;

~~~
- ability to process _expert trajectories_.

---
# Off-policy Prediction

Consider the prediction problem for the off-policy case.

~~~
In order to use episodes from $b$ to estimate values for $π$, we require that
every action taken by $π$ is also taken by $b$, i.e.,
$$π(a|s) > 0 ⇒ b(a|s) > 0.$$

~~~
Many off-policy methods utilize **importance sampling**, a general technique for
estimating expected values of one distribution given samples from another
distribution.

---
# Importance Sampling

Assume that $p$ and $q$ are two distributions and let $x_i$ be $N$ samples of $p$.
According to the [law of large numbers](https://ufal.mff.cuni.cz/~straka/courses/npfl139/2425/slides/?01#23),
we can then estimate $𝔼_{⁇x∼p}\big[f(x)\big]$ as $\frac{1}{N} ∑_i f(x_i)$.

~~~
In order to estimate $𝔼_{⁇x∼q}\big[f(x)\big]$ using the samples $x_i$, we need to account
for different probabilities of $x_i$ under the two distributions. It is
straightforward to verify that
$$𝔼_{⁇x∼q}\big[f(x)\big] = 𝔼_{⁇x∼p}\left[\frac{q(x)}{p(x)}f(x)\right].$$

~~~
Therefore, we can estimate
$$𝔼_{⁇x∼q}\big[f(x)\big] ≈ \frac{1}{N} ∑_i \frac{q(x_i)}{p(x_i)} f(x_i),$$
with $q(x)/p(x)$ being the **relative probability** of $x$ under the two
distributions.

~~~
Both estimates mentioned on this slide are _unbiased_.

---
# Off-policy Prediction

Given an initial state $S_t$ and an episode $A_t, S_{t+1}, A_{t+1}, …, S_T$,
the probability of this episode under a policy $π$ is
$$∏_{k=t}^{T-1} π(A_k | S_k) p(S_{k+1} | S_k, A_k).$$

~~~
Therefore, the relative probability of a trajectory under the target and
the behaviour policy is
$$ρ_t ≝ \frac{∏_{k=t}^{T-1} π(A_k | S_k) p(S_{k+1} | S_k, A_k)}{∏_{k=t}^{T-1} b(A_k | S_k) p(S_{k+1} | S_k, A_k)}
      = ∏_{k=t}^{T-1} \frac{π(A_k | S_k)}{b(A_k | S_k)}.$$

~~~
The $ρ_t$ is usually called the **importance sampling ratio** or _relative
probability_.

~~~
Therefore, if $G_t$ is a return of episode generated according to $b$, we can
estimate
$$v_π(S_t) = 𝔼_b[ρ_t G_t].$$

---
# Off-policy Monte Carlo Prediction

Let $𝓣(s)$ be a set of times when we visited state $s$. Given episodes sampled
according to $b$, we can estimate
$$v_π(s) = \frac{∑_{t∈𝓣(s)} ρ_t G_t}{|𝓣(s)|}.$$

~~~
Such simple average is called **ordinary importance sampling**. It is unbiased, but
can have very high variance.

~~~
An alternative is **weighted importance sampling**, where we compute weighted
average as
$$v_π(s) = \frac{∑_{t∈𝓣(s)} ρ_t G_t}{∑_{t∈𝓣(s)} ρ_t}.$$

~~~
Weighted importance sampling is biased (with bias asymptotically converging to
zero, i.e., a _consistent_ estimate), but has smaller variance.

---
# Off-policy Multi-armed Bandits

![w=30%,f=right](../01/k-armed_bandits.svgz)

As a simple example, consider the 10-armed bandits from the first lecture, with
single-step episodes.

~~~
Let the _behaviour policy_ be a uniform policy, so the return is a reward of
a randomly selected arm.

~~~
Let the _target policy_ be a greedy policy always using action 3.

~~~
Assume that the first sample from the behaviour policy produced action 3 with
reward $R$.
~~~
Then
- Ordinary importance sampling estimates the return for the target policy as
  $$\frac{π(a)}{b(a)} R = \frac{1}{1/10} R = 10⋅R.$$

  The factor $10$ is present because the behaviour policy returns action 3
  in 10% cases.

~~~
- Weighted importance sampling estimates the return for target policy as average
  of rewards for action 3.

---
# Off-policy Monte Carlo Policy Evaluation

![w=76.5%,h=center](importance_sampling.svgz)

Comparison of ordinary and weighted importance sampling on Blackjack. Given
a state with dealer showing a deuce and sum of player's cards 13 with a usable
ace, we estimate the target policy of sticking only with a sum of 20 or 21, using
uniform behaviour policy. Starting from zero estimate, both methods approximate
the target policy value of $≈-0.277$ well in 1k eps.

---
# Infinite Variance of Ordinary Importance Sampling

![w=77%,h=center](importance_sampling_inf_var.svgz)

---
# Infinite Variance of Ordinary Importance Sampling

![w=95%,mw=38%,f=right,h=right](importance_sampling_inf_var.svgz)

In weighted importance sampling, once we sample an episode with non-zero target
probability, our estimate will be always 1.

~~~
In ordinary importance sampling, the variance is infinite. To show that, it
is enough to show that the expectation of the second moment is infinite because
the mean is finite.

~~~
$$𝔼_b \Bigg[\bigg(∏\nolimits_t \frac{π(A_t|S_t)}{b(A_t|S_t)} G_0\bigg)^2\Bigg]$$

~~~
$\displaystyle \kern1em =
  \underbrace{\tfrac{1}{2} \cdot 0.1 \Big(\tfrac{1}{0.5}\Big)^2}_\textrm{episodes of length 1} +
  \underbrace{\tfrac{1}{2} \cdot 0.9 \cdot \tfrac{1}{2} \cdot 0.1 \Big(\tfrac{1}{0.5}\tfrac{1}{0.5}\Big)^2}_\textrm{episodes of length 2} +
  \underbrace{\Big(\tfrac{1}{2} \cdot 0.9\Big)^2 \cdot \tfrac{1}{2} \cdot 0.1 \Big(\tfrac{1}{0.5}\tfrac{1}{0.5}\tfrac{1}{0.5}\Big)^2}_\textrm{episodes of length 3} + \ldots$

~~~
$\displaystyle \kern1em = 0.1 ∑_{k=0}^∞ 0.9^k 2^{k+1} = 0.2 ∑_{k=0}^∞ 1.8^k = ∞.$

---
# Off-policy Monte Carlo Policy Evaluation

We can compute weighted importance sampling similarly to the incremental
implementation of Monte Carlo averaging.

![w=75%,h=center](off_policy_mc_prediction.svgz)

---
# Off-policy Monte Carlo Control

![w=80%,h=center](off_policy_mc.svgz)

---
section: ExSarsa
class: section
# Expected Sarsa

---
# Expected Sarsa

The action $A_{t+1}$ is a source of variance, providing correct estimate only _in expectation_.

~~~
We could improve the algorithm by considering all actions proportionally to their
policy probability, obtaining Expected Sarsa algorithm:
$$\begin{aligned}
  q(S_t, A_t) &← q(S_t, A_t) + α\Big(R_{t+1} + [¬\textrm{done}]⋅γ 𝔼_π q(S_{t+1}, a) - q(S_t, A_t)\Big)\\
              &← q(S_t, A_t) + α\Big(R_{t+1} + [¬\textrm{done}]⋅γ ∑\nolimits_a π(a|S_{t+1}) q(S_{t+1}, a) - q(S_t, A_t)\Big).
\end{aligned}$$

~~~
Compared to Sarsa, the expectation removes a source of variance and therefore
usually performs better. However, the complexity of the algorithm increases and
becomes dependent on number of actions $|𝓐|$.

---
# Expected Sarsa as an Off-policy Algorithm

Note that Expected Sarsa is also an off-policy algorithm, allowing the behaviour
policy $b$ and target policy $π$ to differ.

~~~
Especially, if $π$ is a greedy policy with respect to current value function,
Expected Sarsa simplifies to Q-learning.

---
# Expected Sarsa Example

![w=25%](../02/cliff_walking.svgz)![w=90%,mw=75%,h=center](expected_sarsa.svgz)

Asymptotic performance is an average over 100k episodes (10 runs), interim
performance over the first 100 episodes (50k runs); $ε$-greedy policy with
$ε=0.1%$ is used.

---
section: DoubleQ
class: section
# Maximization Bias and Double Q-learning

---
# Q-learning and Maximization Bias

Because behaviour policy in Q-learning is $ε$-greedy variant of the target
policy, the same samples (up to $ε$-greedy) determine both the maximizing action
and its value estimate.

~~~
![w=75%,h=center](double_q_learning_example.svgz)

---
# Double Q-learning

![w=80%,h=center](double_q_learning.svgz)

---
section: $n$-step
class: section
# The $n$-step Methods

---
# $n$-step Methods

![w=40%,f=right](nstep_td.svgz)

Full return is
$$G_t = ∑_{k=t}^∞ γ^{k-t} R_{k+1},$$
one-step return is
$$G_{t:t+1} = R_{t+1} + [¬\textrm{done}]⋅γ V(S_{t+1}).$$

~~~
We can generalize both into $n$-step returns:
$$G_{t:t+n} ≝ \left(∑_{k=t}^{t+n-1} γ^{k-t} R_{k+1}\right) + γ^n V(S_{t+n}).$$
with $G_{t:t+n} ≝ G_t$ if $t+n ≥ T$ (episode length).

---
# $n$-step Methods

A natural update rule is
$$V(S_t) ← V(S_t) + α\big(G_{t:t+n} - V(S_t)\big).$$

~~~
![w=58%,h=center](nstep_td_prediction.svgz)

---
# $n$-step Methods Example

Using the random walk example, but with 19 states instead of 5,
![w=50%,h=center](../02/td_mc_comparison_example.svgz)

we obtain the following comparison of different values of $n$:
![w=53%,h=center](nstep_td_performance.svgz)

---
# $n$-step Sarsa

Defining the $n$-step return to utilize action-value function as
$$G_{t:t+n} ≝ \left(∑_{k=t}^{t+n-1} γ^{k-t} R_{k+1}\right) + γ^n Q(S_{t+n}, A_{t+n})$$
with $G_{t:t+n} ≝ G_t$ if $t+n ≥ T$,
~~~
we get the following straightforward
algorithm:
$$Q(S_t, A_t) ← Q(S_t, A_t) + α\big(G_{t:t+n} - Q(S_t, A_t)\big).$$

~~~
![w=70%,h=center](nstep_sarsa_example.svgz)

---
# $n$-step Sarsa Algorithm

![w=60%,h=center](nstep_sarsa_algorithm.svgz)

---
# Off-policy $n$-step Sarsa

Recall the relative probability of a trajectory under the target and behaviour policies,
which we now generalize as
$$ρ_{t:t+n} ≝ ∏_{k=t}^{\min(t+n, T-1)} \frac{π(A_k | S_k)}{b(A_k | S_k)}.$$

~~~
Then a simple off-policy $n$-step TD policy evaluation can be computed as
$$V(S_t) ← V(S_t) + αρ_{t:t+n-1}\big(G_{t:t+n} - V(S_t)\big).$$

~~~
Similarly, $n$-step Sarsa becomes
$$Q(S_t, A_t) ← Q(S_t, A_t) + αρ_{\boldsymbol{t+1}:\boldsymbol{t+n}}\big(G_{t:t+n} - Q(S_t, A_t)\big).$$

---
# Off-policy $n$-step Sarsa

![w=60%,h=center](off_policy_nstep_sarsa.svgz)

---
section: Tree Backup
class: section
# Tree Backup:<br>Off-policy $n$-step Without Importance Sampling

---
# Off-policy $n$-step Without Importance Sampling

![w=30%,h=center](off_policy_nstep_algorithms.svgz)

Q-learning and Expected Sarsa can learn off-policy without importance sampling.

~~~
To generalize to $n$-step off-policy method, we must compute expectations
over actions in each step of $n$-step update. However, we have not obtained
a return for the non-sampled actions.

~~~
Luckily, we can estimate their values by using the current action-value
function.

---
# Off-policy $n$-step Without Importance Sampling

![w=10%,f=right](tree_backup_example.svgz)

~~~
We now derive the $n$-step reward, starting from one-step:
$$G_{t:t+1} ≝ R_{t+1} + [¬\textrm{done}]⋅γ∑\nolimits_a π(a|S_{t+1}) Q(S_{t+1}, a).$$

~~~
For two-step, we get:
$$G_{t:t+2} ≝ R_{t+1} + γ∑\nolimits_{a≠A_{t+1}} π(a|S_{t+1}) Q(S_{t+1}, a) + γπ(A_{t+1}|S_{t+1})G_{t+1:t+2}.$$

~~~
Therefore, we can generalize to:
$$G_{t:t+n} ≝ R_{t+1} + γ∑\nolimits_{a≠A_{t+1}} π(a|S_{t+1}) Q(S_{t+1}, a) + γπ(A_{t+1}|S_{t+1})G_{t+1:t+n},$$
with $G_{t:t+n} ≝ G_{t:T}$ if $t+n ≥ T$ (episode length).

~~~
The resulting algorithm is $n$-step **Tree backup** and it is an off-policy
$n$-step temporal difference method not requiring importance sampling.

---
# Off-policy $n$-step Without Importance Sampling

![w=55%,h=center](tree_backup_algorithm.svgz)

---
section: Refresh
# Summary

- Until now, we have solved the tasks by explicitly estimating expected return,
either as $v(s)$ or as $q(s, a)$.
~~~

  - Finite number of states and actions.
~~~
  - We do not share information between different states or actions.
~~~
  - We use $q(s, a)$ if we do not have the environment model
    (a _model-free_ method); if we do, it is usually better to
    estimate $v(s)$ and choose actions as $\argmax\nolimits_a 𝔼[R + v(s')]$.
~~~
- The methods we discussed so far differ in several aspects:

  - Whether they compute return by simulating a whole episode (Monte Carlo
    methods), or by bootstrapping (temporal difference, i.e.,
    $G_t ≈ R_t + v(S_t)$, possibly $n$-step).
~~~
    - TD methods are more noisy and unstable, but can learn immediately and
      explicitly assume Markovian property of value function.
~~~
  - Whether they estimate the value function of the same policy they use to
    generate the episodes (on-policy) or not (off-policy).
~~~
    - The off-policy methods are more noisy and unstable, but more flexible.

---
section: FunApprox
class: section

# Function Approximation

<div style="position: absolute; left: 0px; right: 0px; text-align: center"><i>Applause please: neural networks arrive at the scene.</i></div>

---
# Function Approximation

We now approximate the value function $v$ and/or the action-value function $q$,
selecting it from a family of functions parametrized by a weight vector $→w ∈ ℝ^d$.

~~~
We denote the approximations as
$$\begin{gathered}
  v̂(s; →w),\\
  q̂(s, a; →w).
\end{gathered}$$

~~~
Weights are usually shared among states. Therefore, we need to define state
distribution $μ(s)$ to obtain an objective for finding the best function approximation
(if we give preference to some states, improving their estimates might worsen
estimates in other states).

~~~
The state distribution $μ(s)$ gives rise to a natural objective function called
**Mean Squared Value Error**, denoted $\overline{VE}$:
$$\overline{VE}(→w) ≝ ∑_{s∈𝓢} μ(s) \big(v_π(s) - v̂(s; →w)\big)^2.$$

---
# Function Approximation

For on-policy algorithms, $μ(s)$ is often the on-policy distribution (fraction of
time spent in $s$).

~~~
- For **episodic tasks**, let $h(s)$ be the probability that an episodes starts in state $s$,
  and let $η(s)$ denote the number of time steps spent, on average, in state $s$
  in a single episode:
  $$η(s) = h(s) + ∑\nolimits_{s'}η(s')∑\nolimits_a π(a|s') p(s|s', a).$$

~~~
  The on-policy distribution is then obtained by normalizing: $μ(s) ≝ \frac{η(s)}{∑_{s'} η(s')}.$

~~~
  ![w=30%,f=right](discounting_as_termination.svgz)

  If there is discounting ($γ<1$), it should be treated as a form of
  termination, by including a factor $γ$ to the second term of the $η(s)$ equation.

~~~
- For **continuing tasks**, we require $γ<1$, and employ the same definition as
  in the episodic case.

---
# Gradient and Semi-Gradient Methods

The functional approximation (i.e., the weight vector $→w$) is usually optimized
using gradient methods, for example as
$$\begin{aligned}
  →w_{t+1} &← →w_t - \tfrac{1}{2} α ∇_{→w_t} \big(v_π(S_t) - v̂(S_t; →w_t)\big)^2\\
           &← →w_t + α\big(v_π(S_t) - v̂(S_t; →w_t)\big) ∇_{→w_t} v̂(S_t; →w_t).\\
\end{aligned}$$

~~~
As usual, the $v_π(S_t)$ is estimated by a suitable sample of a return:
~~~
- in Monte Carlo methods, we use episodic return $G_t$,
~~~
- in temporal difference methods, we employ bootstrapping and use
  one-step return
  $$R_{t+1} + [¬\textrm{done}]⋅γv̂(S_{t+1}; →w)$$
  or an $n$-step return.

---
class: middle
# Monte Carlo Gradient Policy Evaluation

![w=100%](../03/grad_mc_estimation.svgz)

If the return estimate $G_t$ is unbiased (which it is in a Monte Carlo method),
the policy evaluation algorithm is guaranteed to converge to a local optimum of
the mean squared value error under the usual SGD conditions.

---
# Linear Methods

A simple special case of function approximation are linear methods, where
$$v̂\big(→x(s); →w\big) ≝ →x(s)^T →w = ∑x(s)_i w_i.$$

~~~
The $→x(s)$ is a representation of state $s$, which is a vector of the same size
as $→w$. It is sometimes called a _feature vector_.

~~~
The SGD update rule then becomes
$$→w_{t+1} ← →w_t + α\big(v_π(S_t) - v̂(→x(S_t); →w_t)\big) →x(S_t).$$

~~~
This rule is the same as in the tabular methods if $→x(s)$ is the one-hot
representation of the state $s$.

---
# State Aggregation

Simple way of generating a feature vector is **state aggregation**, where several
neighboring states are grouped together.

~~~
For example, consider a 1000-state random walk, where transitions lead uniformly
randomly to any of 100 neighboring states on the left or on the right. Using
state aggregation, we can partition the 1000 states into 10 groups of 100
states. Monte Carlo policy evaluation then computes the following:

![w=60%,h=center](grad_mc_estimation_example.svgz)

---
# Feature Construction for Linear Methods

Many methods for construction features for linear methods have been developed in the past:
~~~
- polynomials,

~~~
- Fourier bases,
~~~
- radial basis functions,
~~~
- tile coding,
~~~
- …

~~~
But of course, nowadays we use deep neural networks, which construct a suitable
feature vector automatically as a latent variable (the last hidden layer).

---
section: Tile Coding
class: section
# Tile Coding

---
# Tile Coding

![w=100%,mh=90%,v=middle](tile_coding.svgz)

~~~
If $t$ overlapping tiles are used, the learning rate is usually normalized as $α/t$.

---
# Tile Coding

For example, on the 1000-state random walk example, the performance of the tile
coding surpasses state aggregation:

![w=60%,h=center](tile_coding_performance.svgz)
Each tile covers 200 states, and when multiple tiles are used, they are offset
by 4 states.

---
# Asymmetrical Tile Coding

In higher dimensions, the tiles should have asymmetrical offsets, with
a sequence of $(1, 3, 5, …, 2d-1)$ proposed as a good choice.

![w=50%,h=center](tile_coding_asymmetrical.svgz)
