title: NPFL139, Lecture 5
class: title, langtech, cc-by-sa
# Distributional RL,<br>Policy Gradient Methods

## Milan Straka

### March 19, 2025

---
class: section
# Refresh

---
# Refresh

## Distributional RL

Instead of an expected return $Q(s, a)$, we could estimate the distribution of
expected returns $Z(s, a)$ – the _value distribution_.

~~~
The authors define the distributional Bellman operator $𝓣^π$ as:
$$𝓣^π Z(s, a) ≝ R(s, a) + γ Z(S', A')~~~\textrm{for}~~~S'∼p(s, a), A'∼π(S').$$

~~~
The authors of the paper prove similar properties of the distributional Bellman
operator compared to the regular Bellman operator, mainly being a contraction
under a suitable metric.
~~~
- For Wasserstein metric $W_p$, the authors define  
  $$W̄_p(Z_1, Z_2)≝\sup\nolimits_{s, a} W_p\big(Z_1(s, a), Z_2(s, a)\big)$$
  and prove that $𝓣^π$ is a γ-contraction in $W̄_p$.
~~~
- However, $𝓣^π$ is not a contraction in KL divergence nor in total variation
  distance.

---
style: .katex-display { margin: .2em 0 }
class: dbend
# Refresh – Wasserstein Metric

For two probability distributions $μ, ν$ on a metric space with metric $d$,
Wasserstein metric $W_p$ is defined as
$$W_p(μ, ν) ≝ \inf_{γ∈Γ(μ,ν)} \Big(𝔼_{(x, y)∼γ} d\big(x, y\big)^p\Big)^{1/p},$$
~~~
where $Γ(μ,ν)$ is a set of all _couplings_, each being a joint probability
distribution whose marginals are $μ$ and $ν$, respectively.
~~~
A possible intuition is the optimal transport of probability mass from $μ$ to
$ν$.

~~~
For distributions over reals with CDFs $F, G$, the optimal transport has an
analytic solution:

![w=27.5%,f=right](../05/wasserstein-1.svgz)

$$W_p(μ, ν) = \bigg(∫\nolimits_0^1 |F^{-1}(q) - G^{-1}(q)|^p \d q\bigg)^{1/p},$$
where $F^{-1}$ and $G^{-1}$ are _quantile functions_, i.e., inverse CDFs.

~~~
For $p=1$, the 1-Wasserstein metric correspond to area “between” F and G, and
in that case we can compute it also as $W_1(μ, ν) = ∫\nolimits_x \big|F(x)- G(x)\big| \d x.$

---
class: middle
# Refresh – Wasserstein Metric

![w=50%](../05/wasserstein_1d.png)

![w=49%,f=right](../05/wasserstein_vs_ks_categorical.svgz)

![w=50%](../05/wasserstein_vs_kl.png)


---
# Refresh – C51

## Distributional RL: C51

The distribution of returns is modeled as a discrete distribution parametrized
by the number of atoms $N ∈ ℕ$ and by $V_\textrm{MIN}, V_\textrm{MAX} ∈ ℝ$.
Support of the distribution are atoms
$$\{z_i ≝ V_\textrm{MIN} + i Δz : 0 ≤ i < N\}\textrm{~~~for~}Δz ≝ \frac{V_\textrm{MAX} - V_\textrm{MIN}}{N-1}.$$

~~~
The atom probabilities are predicted using a $\softmax$ distribution as
$$Z_{→θ}(s, a) = \left\{z_i\textrm{ with probability }p_i = \frac{e^{f_i(s, a; →θ)}}{∑_j e^{f_j(s, a; →θ)}}\right\}.$$

---
# Refresh – C51

## Distributional RL: C51

![w=30%,f=right](../05/dqn_distributional_operator.svgz)

After the Bellman update, the support of the distribution $R(s, a) + γZ(s', a')$
is not the same as the original support. We therefore project it to the original
support by proportionally mapping each atom of the Bellman update to immediate
neighbors in the original support.

~~~
$$Φ\big(R(s, a) + γZ(s', a')\big)_i ≝
  ∑_{j=1}^N \left[ 1 - \frac{\left|[r + γz_j]_{V_\textrm{MIN}}^{V_\textrm{MAX}}-z_i\right|}{Δz} \right]_0^1 p_j(s', a').$$

~~~
The network is trained to minimize the Kullbeck-Leibler divergence between the
current distribution and the (mapped) distribution of the one-step update
$$D_\textrm{KL}\Big(Φ\big(R + γZ_{→θ̄}\big(s', \argmax_{a'} 𝔼Z_{→θ̄}(s', a')\big)\big) \Big\| Z_{→θ}\big(s, a\big)\Big).$$

---
# Refresh – C51

## Distributional RL: C51

![w=45%,h=center](../05/dqn_distributional_algorithm.svgz)

Note that by minimizing the $D_\textrm{KL}$ instead of the Wasserstein metric
$W_p$, the algorithm has no guarantee of convergence of any kind. However, the
authors did not know how to minimize it.

---
section: QR
class: section
# Quantile Regression

---
# Distributional RL with Quantile Regression

Although the authors of C51 proved that the distributional Bellman operator
is a contraction with respect to Wasserstein metric $W_p$, they were not able
to actually minimize it during training; instead, they minimize the KL
divergence between the current value distribution and one-step estimate.

![w=60%,h=center](qr_dqn_c51projection.svgz)

---
# Distributional RL with Quantile Regression

The same authors later proposed a different approach, which actually manages to minimize
the 1-Wasserstein distance.

~~~
In contrast to C51, where $Z(s, a)$ is represented using a discrete distribution
on a fixed “comb” support of uniformly spaces locations, we now represent it
as a _quantile distribution_ – as quantiles $θ_i(s, a)$ for a fixed
probabilities $τ_1, …, τ_N$ with $τ_i = \frac{i}{N}$.

~~~
![w=37%,f=right](qr_dqn_1wasserstein.svgz)

Formally, we can define the quantile distribution as a uniform combination of
$N$ Diracs:
$$Z_θ(s, a) ≝ \frac{1}{N} ∑_{i=1}^N δ_{θ_i(s, a)},$$
~~~
so that the cumulative density function is a step function increasing by
$\frac{1}{N}$ on every quantile $θ_i$.

---
# Distributional RL with Quantile Regression

The quantile distribution offers several advantages:

~~~
- a fixed support is no longer required;

~~~
- the projection step $Φ$ is not longer needed;

~~~
- this parametrization enables direct minimization of the Wasserstein loss.

---
# Distributional RL with Quantile Regression

Recall that 1-Wasserstein distance between two distributions $μ, ν$ can be computed as
$$W_1(μ, ν) = ∫\nolimits_0^1 \big|F_μ^{-1}(q) - F_ν^{-1}(q)\big| \d q,$$
where $F_μ$, $F_ν$ are their cumulative density functions.

~~~
For arbitrary distribution $Z$, the we denote the most accurate quantile
distribution as
$$Π_{W_1} Z ≝ \argmin_{Z_θ} W_1(Z, Z_θ).$$

~~~
In this case, the 1-Wasserstein distance can be written as
$$W_1(Z, Z_θ) = ∑_{i=1}^N ∫\nolimits_{τ_{i-1}}^{τ_i} \big|F_Z^{-1}(q) - θ_i\big| \d q.$$

---
# Distributional RL with Quantile Regression

It can be proven that for continuous $F_Z^{-1}$, $W_1(Z, Z_θ)$ is minimized by
(for proof, see Lemma 2 of Dabney et al.: Distributional Reinforcement Learning
with Quantile Regression, or consider how the 1-Wasserstein distance changes in
the range $[τ_{i-1}, τ_i]$ when you move $θ_i$):

![w=46%,f=right](qr_dqn_1wasserstein.svgz)

$$\bigg\{θ_i ∈ ℝ \bigg| F_Z(θ_i) = \frac{τ_{i-1} + τ_i}{2}\bigg\}.$$

~~~
We denote the _quantile midpoints_ as
$$τ̂_i ≝ \frac{τ_{i-1} + τ_i}{2}.$$

~~~
In the paper, the authors prove that the composition
$Π_{W_1} 𝓣^π$ is γ-contraction in $W̄_∞$, so repeated
application of $Π_{W_1} 𝓣^π$ converges to a unique fixed
point.

---
# Quantile Regression

Our goal is now to show that it is possible to estimate a quantile $τ ∈ [0, 1]$
by minimizing a loss suitable for SGD.

~~~
Assume we have samples from a distribution $P$.

~~~
- Minimizing the MSE of $x̂$ and the samples of $P$,
  $$x̃ = \argmin\nolimits_x̂\, 𝔼_{x∼P} \big[(x - x̂)^2\big],$$
  yields the _mean_ of the distribution, $x̃ = 𝔼_{x∼P}[x]$.

~~~
  To show that this holds, we compute the derivative of the loss with respect to
  $x̂$ and set it to 0, arriving at
  $$0 = 𝔼_x [2(x̂ - x)] = 2 𝔼_x[x̂] - 2𝔼_x[x] = 2\big(x̂ - 𝔼_x[x]\big).$$

---
# Quantile Regression

Assume we have samples from a distribution $P$ with cumulative density function
$F_P$.

- Minimizing the mean absolute error (MAE) of $x̂$ and the samples of $P$,
  $$x̃ = \argmin\nolimits_x̂\, 𝔼_{x∼P} \big[|x - x̂|\big],$$
~~~
  yields the _median_ of the distribution, $x̃ = F_P^{-1}(0.5)$.

~~~
  We prove this again by computing the derivative with respect to $x̂$, assuming
  the functions are nice enough that the Leibnitz integral rule can be used:

~~~
  $\displaystyle \frac{∂}{∂x̂} ∫_{-∞}^{∞} P(x) |x - x̂| \d x = \frac{∂}{∂x̂} \bigg[∫_{-∞}^{x̂} P(x) (x̂ - x) \d x  + ∫_x̂^∞ P(x) (x - x̂) \d x \bigg]$

~~~
  $\displaystyle \hphantom{\frac{∂}{∂x̂} ∫_{-∞}^{∞} P(x) |x̂ - x| \d x} = ∫_{-∞}^{x̂} P(x) \d x - ∫_x̂^∞ P(x) \d x$

~~~
  $\displaystyle \hphantom{\frac{∂}{∂x̂} ∫_{-∞}^{∞} P(x) |x̂ - x| \d x} = 2 ∫_{-∞}^{x̂} P(x) \d x - 1 = 2 F_P(x̂) - 1 = 2 \big(F_P(x̂) - \tfrac{1}{2}\big).$

---
class: dbend
# Leibniz integral rule

The Leibniz integral rule for differentiation under the integral sign states that
for $-∞ < a(x), b(x) < ∞$,

$$\frac{∂}{∂ x} \bigg[∫_{a(x)}^{b(x)} f(x, t) \d t \bigg] =$$

~~~
$$ = ∫_{a(x)}^{b(x)} \frac{∂}{∂ x} f(x, t) \d t
   + \bigg(\frac{∂}{∂ x} b(x)\bigg) f\big(x, b(x)\big)
   - \bigg(\frac{∂}{∂ x} a(x)\bigg) f\big(x, a(x)\big).$$

~~~
_Sufficient condition for the Leibnitz integral rule to hold is that the $f(x,
y)$ and its partial derivative $\frac{∂}{∂x}f(x, y)$ are continuous in both $x$
and $t$, and $a(x)$ and $b(x)$ are continuous and have continuous derivatives._

~~~
_If any of the bounds is improper, additional conditions must hold, notably that
the integral of the partial derivatives of $f$ must converge._

---
# Quantile Regression

Assume we have samples from a distribution $P$ with cumulative density function
$F_P$.

- By generalizing the previous result, we can show that for a quantile $τ ∈ [0,
  1]$, if
  $$x̃ = \argmin\nolimits_x̂\, 𝔼_{x∼P} \big[(x - x̂)(τ - [x ≤ x̂])\big],$$
  then $x̃ = F_P^{-1}(τ)$.
~~~
  Let $ρ_τ(x - x̂) ≝ (x - x̂)(τ - [x ≤ x̂]) = |x - x̂| ⋅ |τ - [x ≤ x̂]|$.
~~~
  This loss penalizes overestimation errors with weight $1-τ$, underestimation
  errors with $τ$.

~~~
  $\displaystyle \frac{∂}{∂x̂} ∫_{-∞}^{∞} P(x) (x - x̂)(τ - [x ≤ x̂]) \d x =$

~~~
  $\displaystyle \kern2em = \frac{∂}{∂x̂} \bigg[(τ-1) ∫_{-∞}^{x̂} P(x) (x - x̂) \d x + τ ∫_x̂^∞ P(x) (x - x̂) \d x \bigg]$

~~~
  $\displaystyle \kern2em = (\textcolor{blue}{1} - \textcolor{magenta}{τ}) ∫_{-∞}^{x̂} P(x) \d x - \textcolor{magenta}{τ} ∫_x̂^∞ P(x) \d x = \textcolor{blue}{∫_{-∞}^{x̂} P(x) \d x} - \textcolor{magenta}{τ} = F_P(x̂) - τ.$


---
# Quantile Regression

Using the quantile regression, when we have a value distribution $Z$, we can
find the most accurate quantile distribution by minimizing
$$∑_{i=1}^N 𝔼_{z ∼ Z} \big[ρ_{τ̂_i}(z - θ_i)\big].$$

~~~
However, the quantile loss is not smooth around zero, which could limit
performance when training a model. The authors therefore propose the
**quantile Huber loss**, which acts as an asymmetric squared loss
in interval $[-κ, κ]$ and fall backs to the standard quantile loss outside this
range.

~~~
Specifically, let
$$ρ_τ^κ(z - θ) ≝ \begin{cases}
  \big|τ - [z ≤ θ]\big| ⋅ \tfrac{1}{2}\big(z - θ\big)^2 &~~\textrm{if}~~ |z - θ| ≤ κ,\\
  \big|τ - [z ≤ θ]\big| ⋅ κ\big(|z - θ| - \tfrac{1}{2}κ\big) &~~\textrm{otherwise}.\\
  \end{cases}$$
$$

---
# Distributional RL with Quantile Regression

To conclude, in DR-DQN-$κ$, the network for a given state predicts $ℝ^{|𝓐|×N}$,
so $N$ quantiles for every action.

~~~
The following loss is used:

![w=65%,h=center](qr_dqn_loss.svgz)

The $q_j$ is just $\frac{1}{N}$.

---
# Distributional RL with Quantile Regression

![w=100%](qr_dqn_approximation_errors.svgz)

Each state transition has probability of 0.1 of moving in a random direction.

---
# Distributional RL with Quantile Regression

![w=90%,h=center](qr_dqn_atari_graphs.svgz)

![w=37%,h=center](qr_dqn_atari_results.svgz)

---
section: IQN
class: section
# Implicit Quantile Regression

---
# Implicit Quantile Networks for Distributional RL

In IQN (implicit quantile regression), the authors (again the same team as in
C51 and DR-DQN) generalize the value distribution representation to predict
_any given quantile $τ$_.

![w=69%,f=right](iqn_architecture_comparison.svgz)

~~~
- The $ψ(s)$ is a convolutional stack from DQN, composed of

  - CNN $8×8$, stride 4, 32 filters, ReLU;
  - CNN $4×4$, stride 2, 64 filters, ReLU;
  - CNN $3×3$, stride 1, 64 filters, ReLU.

~~~
- The $f$ is an MLP:
  - fully connected layer with 512 units, ReLU;
  - output layer, 1 unit.

---
style: .katex-display { margin: .7em 0pt }
# Implicit Quantile Networks for Distributional RL

The quantile $τ$ of the value distribution, $Z_τ(s, a)$, is modeled as
$$Z_τ(s, a) ≈ f\big(ψ(s) ⊙ φ(τ)\big)_a.$$

~~~
- Other ways than multiplicative combinations were tried (concatenation, or
  residual computation $ψ(s)⊙(1+φ(τ))$), but the multiplicative form delivered
  the best results.

~~~
- The quantile $τ$ is represented using trainable cosine embeddings with
  dimension $n=64$:
  $$φ_j(τ) ≝ \operatorname{ReLU}\Big(∑\nolimits_{i=0}^{n-1} \cos(π i τ) w_{i,j} + b_j\Big).$$

~~~
- The target policy is greedy with respect to action-value approximation
  computed using $K$ samples $τ̃_k ∼ U[0, 1]$:
  $$π(x) ≝ \argmax_a \frac{1}{K} ∑_{k=1}^K Z_{τ̃_k}(x, a).$$
~~~
  - As in DQN, the exploration is still performed by using the $ε$-greedy
    policy.

---
# Implicit Quantile Networks for Distributional RL

The overall loss is:

![w=60%,h=center](iqn_loss.svgz)

~~~

Note the different roles of $N$ and $N'$.

---
# Implicit Quantile Networks for Distributional RL

![w=95%,mw=67%,f=right,h=right](iqn_n_ablations.svgz)

The authors speculate that:
- large $N$ may increase sample complexity (faster
learning because we have more loss terms),
- larger $N'$ could reduce variance (like a minibatch size).

---
# Implicit Quantile Networks for Distributional RL

![w=75%,h=center](iqn_atari_graphs.svgz)
![w=52%,h=center,mw=65%](iqn_atari_results.svgz)![w=100%,mw=35%](iqn_atari_results_2.svgz)

---
# Implicit Quantile Networks for Distributional RL

The ablation experiments of the quantile representation. A full grid search with
two seeds for every configuration was performed, with the black dots
corresponding to the hyperparameters of IQN; six Atari games took part in the
evaluation.

![w=92%,h=center](iqn_hyperparameters.svgz)

~~~
- the gray horizontal line is the QR-DQN baseline;
~~~
- “learn” is a learnt MLP embedding with a single hidden layer of size $n$;
~~~
- “concat” combines the state and quantile representations by concatenation, not
  $⊙$.

---
# TrackMania using Implicit Quantile Networks

![w=89.5%,h=center](iqn_trackmania.jpg)

---
section: Policy Gradient Methods
class: section
# Policy Gradient Methods

---
# Policy Gradient Methods

Instead of predicting expected returns, we could train the method to directly
predict the policy
$$π(a | s; →θ).$$

~~~
Obtaining the full distribution over all actions would also allow us to sample
the actions according to the distribution $π$ instead of just $ε$-greedy
sampling.

~~~
However, to train the network, we maximize the expected return $v_π(s)$ and to
that account we need to compute its _gradient_ $∇_{→θ} v_π(s)$.

---
# Policy Gradient Methods

In addition to discarding $ε$-greedy action selection, policy gradient methods
allow producing policies which are by nature stochastic, as in card games with
imperfect information, while the action-value methods have no natural way of
finding stochastic policies (distributional RL might be of some use though).

~~~
![w=75%,f=right](stochastic_policy_example.svgz)

In the example, the reward is -1 per step, and we assume the three states appear
identical under the function approximation.

---
# Policy Gradient Theorem

Let $π(a | s; →θ)$ be a parametrized policy. We denote the initial state
distribution as $h(s)$ and the on-policy distribution under $π$ as $μ(s)$.
Let also $J(→θ) ≝ 𝔼_{s∼h} v_π(s)$.

~~~
Then
$$∇_{→θ} v_π(s) ∝ ∑_{s'∈𝓢} P(s → … → s'|π) ∑_{a ∈ 𝓐} q_π(s', a) ∇_{→θ} π(a | s'; →θ)$$
and
$$∇_{→θ} J(→θ) ∝ ∑_{s∈𝓢} μ(s) ∑_{a ∈ 𝓐} q_π(s, a) ∇_{→θ} π(a | s; →θ),$$

~~~
where $P(s → … → s'|π)$ is the probability of getting to state $s'$ when starting
from state $s$, after any number of 0, 1, … steps. The $γ$ parameter should
be treated as a form of termination, i.e., $P(s → … → s'|π) ∝ ∑_{k=0}^∞ γ^k P(s → s'\textrm{~in~}k\textrm{~steps~}|π)$.

---
# Proof of Policy Gradient Theorem

$\displaystyle ∇v_π(s) = ∇ \Big[ ∑\nolimits_a π(a|s; →θ) q_π(s, a) \Big]$

~~~
$\displaystyle \phantom{∇v_π(s)} = ∑\nolimits_a \Big[ q_π(s, a) ∇ π(a|s; →θ) + π(a|s; →θ) ∇ q_π(s, a) \Big]$

~~~
$\displaystyle \phantom{∇v_π(s)} = ∑\nolimits_a \Big[ q_π(s, a) ∇ π(a|s; →θ) + π(a|s; →θ) ∇ \big(∑\nolimits_{s', r} p(s', r|s, a)(r + γv_π(s'))\big) \Big]$

~~~
$\displaystyle \phantom{∇v_π(s)} = ∑\nolimits_a \Big[ q_π(s, a) ∇ π(a|s; →θ) + γπ(a|s; →θ) \big(∑\nolimits_{s'} p(s'|s, a) ∇ v_π(s')\big) \Big]$

~~~
_We now expand $v_π(s')$._

~~~
$\displaystyle \phantom{∇v_π(s)} = ∑\nolimits_a \Big[ q_π(s, a) ∇ π(a|s; →θ) + γπ(a|s; →θ) \Big(∑\nolimits_{s'} p(s'|s, a)\Big(\\
                \quad\qquad\qquad ∑\nolimits_{a'} \Big[ q_π(s', a') ∇ π(a'|s'; →θ) + γπ(a'|s'; →θ) \big(∑\nolimits_{s''} p(s''|s', a') ∇ v_π(s'')\big) \Big] \Big) \Big) \Big]$

~~~
_Continuing to expand all $v_π(s'')$, we obtain the following:_

$\displaystyle ∇v_π(s) = ∑_{s'∈𝓢} ∑_{k=0}^∞ γ^k P(s → s'\textrm{~in~}k\textrm{~steps~}|π) ∑_{a ∈ 𝓐} q_π(s', a) ∇_{→θ} π(a | s'; →θ).$

---
# Proof of Policy Gradient Theorem

To finish the proof of the first part, recall that
$$∑_{k=0}^∞ γ^k P(s → s'\textrm{~in~}k\textrm{~steps~}|π) ∝ P(s → … → s'|π).$$

~~~
For the second part, we know that
$$∇_{→θ} J(→θ) = 𝔼_{s ∼ h} ∇_{→θ} v_π(s) ∝ 𝔼_{s ∼ h} ∑_{s'∈𝓢} P(s → … → s'|π) ∑_{a ∈ 𝓐} q_π(s', a) ∇_{→θ} π(a | s'; →θ),$$
~~~
therefore using the fact that $μ(s') = 𝔼_{s ∼ h} P(s → … → s'|π)$ we get
$$∇_{→θ} J(→θ) ∝ ∑_{s∈𝓢} μ(s) ∑_{a ∈ 𝓐} q_π(s, a) ∇_{→θ} π(a | s; →θ).$$

---
section: REINFORCE
# REINFORCE Algorithm

The REINFORCE algorithm (Williams, 1992) uses directly the policy gradient
theorem, minimizing $-J(→θ) ≝ -𝔼_{s∼h} v_π(s)$. The loss gradient is then
$$∇_{→θ} -J(→θ) ∝ -∑_{s∈𝓢} μ(s) ∑_{a ∈ 𝓐} q_π(s, a) ∇_{→θ} π(a | s; →θ) = -𝔼_{s ∼ μ} ∑_{a ∈ 𝓐} q_π(s, a) ∇_{→θ} π(a | s; →θ).$$

~~~
However, the sum over all actions is problematic. Instead, we rewrite it to an
expectation which we can estimate by sampling:
$$∇_{→θ} -J(→θ) ∝ 𝔼_{s ∼ μ} 𝔼_{a ∼ π} q_π(s, a) ∇_{→θ} -\ln π(a | s; →θ),$$
where we used the fact that
$$∇_{→θ} \ln π(a | s; →θ) = \frac{1}{π(a | s; →θ)} ∇_{→θ} π(a | s; →θ).$$

---
# REINFORCE Algorithm

REINFORCE therefore minimizes the loss $-J(→θ)$ with gradient
$$𝔼_{s ∼ μ} 𝔼_{a ∼ π} q_π(s, a) ∇_{→θ} -\ln π(a | s; →θ),$$
where we estimate the $q_π(s, a)$ by a single sample.

Note that the loss is just a weighted variant of negative log-likelihood (NLL),
where the sampled actions play a role of gold labels and are weighted according
to their return.

![w=75%,h=center](reinforce.svgz)

---
# REINFORCE Algorithm Example Performance

![w=30%,v=middle](stochastic_policy_example.svgz)![w=69%,v=middle](reinforce_performance.svgz)

---
# On-policy Distribution in REINFORCE

In the proof, we assumed $γ$ is used as a form of termination in the definition
of the on-policy distribution.

~~~
However, even when discounting is used during training (to guarantee convergence
even for very long episodes), evaluation is often performed without discounting.

~~~
Consequently, the distribution $μ$ used in the REINFORCE algorithm is almost
always the unterminated (undiscounted) on-policy distribution (I am not
aware of any implementation or paper that would use the discounted one), so that
we learn even in states that are far from the beginning of an episode.

~~~
Note that this is actually true even for DQN and its variants. Therefore,
the discounting parameter $γ$ is used mostly as a variance-reduction technique.

---
section: Baseline
# REINFORCE with Baseline

The returns can be arbitrary – better-than-average and worse-than-average
returns cannot be recognized from the absolute value of the return.

~~~
Hopefully, we can generalize the policy gradient theorem using a baseline $b(s)$
to
$$∇_{→θ} J(→θ) ∝ ∑_{s∈𝓢} μ(s) ∑_{a ∈ 𝓐} \big(q_π(s, a) - b(s)\big) ∇_{→θ} π(a | s; →θ).$$

~~~
The baseline $b(s)$ can be a function or even a random variable, as long as it
does not depend on $a$, because
$$∑_a b(s) ∇_{→θ} π(a | s; →θ) = b(s) ∑_a ∇_{→θ} π(a | s; →θ) = b(s) ∇_{→θ} ∑_a π(a | s; →θ) = b(s) ∇_{→θ} 1 = 0.$$

---
# REINFORCE with Baseline

A good choice for $b(s)$ is $v_π(s)$, which can be shown to minimize the
variance of the gradient estimate (in the limit $γ → 1$; see L. Weaver and N. Tao,
[The Optimal Reward Baseline for Gradient-Based Reinforcement Learning,
https://arxiv.org/abs/1301.2315](https://arxiv.org/abs/1301.2315), for the proof).
Such baseline reminds centering of returns, given that
$$v_π(s) = 𝔼_{a ∼ π} q_π(s, a).$$

~~~
Then, better-than-average returns are positive and worse-than-average returns
are negative.

~~~
The resulting $q_π(s, a) - v_π(s)$ function is also called the **advantage** function
$$a_π(s, a) ≝ q_π(s, a) - v_π(s).$$

~~~
Of course, the $v_π(s)$ baseline can be only approximated. If neural networks
are used to estimate $π(a|s; →θ)$, then some part of the network is usually
shared between the policy and value function estimation, which is trained using
mean square error of the predicted and observed return.

---
# REINFORCE with Baseline

In REINFORCE with baseline, we train:
1. the _policy network_ using the REINFORCE algorithm, and
~~~
2. the _value network_ by minimizing the mean squared value error
   $\overline{VE}$.

![w=85%,h=center](reinforce_with_baseline.svgz)

---
# REINFORCE with Baseline Example Performance


![w=40%,h=center,mh=48%](stochastic_policy_example.svgz)

![w=48%](reinforce_performance.svgz)![w=52%](reinforce_with_baseline_comparison.svgz)

---
section: OP-REINFORCE
# Operator View of Policy Gradient Methods

In the middle of 2020, _Dibya Ghosh et al._ introduced the operator view of
policy gradient methods in their paper
[An operator view of policy gradient methods, https://arxiv.org/abs/2006.11266](https://arxiv.org/abs/2006.11266).

~~~
## Trajectory Formulation

Let $τ = (S_0, A_0, S_1, A_1, …)$ be a specific trajectory with return
$G(τ) = ∑_{k=0}^∞ γ^k R_{k+1}(τ)$.
~~~
The probability of $τ$ under a policy $π$ is $π(τ) = h(S_0) ∏_i π(A_i | S_i)
p(S_{i+1} | S_i, A_i)$.

~~~
Our goal is then to find
$$→θ^* = \argmax_{→θ} 𝔼_{τ ∼ π_{→θ}} \big[G(τ)\big] = \argmax_{→θ} ∫_τ π(τ) G(τ) \d τ,$$
~~~
and the REINFORCE algorithm at each step sets the weights $→θ_{t+1}$ to
~~~
$$→θ_t + α𝔼_{τ ∼ π_{→θ_t}}\bigg[G(τ) \frac{∂\log π_{→θ}(τ)}{∂→θ}\Big|_{→θ=→θ_t} \bigg]
= →θ_t + α∫_τ π_{→θ_t}(τ) G(τ) \frac{∂\log π_{→θ}(τ)}{∂→θ}\Big|_{→θ=→θ_t} \d τ.$$

---
# Trajector Formulation of OP-REINFORCE

In the operator view, policy improvement is achieved by a successive application
of a **policy improvement operator** $𝓘$ and a **projection operator** $𝓟$. For
tabular methods, the projection operator is identity, but it is needed for
functional approximation methods.

~~~
The operator version of REINFORCE is then the iterative application of $𝓟∘𝓘$
with
$$(𝓘 π)(τ) \stackrel{\tiny\textrm{def}}{∝} G(τ) π(τ),$$
~~~
$$𝓟 ν ≝ \argmin_{→θ} D_\textrm{KL}\big(ν \| π_{→θ}\big).$$

~~~
As formulated, the operator version of REINFORCE computes the projection
perfectly in each step, while the REINFORCE performs just one step
of gradient descent in the direction of $𝓟$. However, it is easy to show that
the fixed points of both algorithms are the same.

---
# Trajector Formulation of OP-REINFORCE

The proposition is actually not difficult to prove, we just need to expand the
definitions.

~~~
Denoting $ν$ the distribution over trajectories such that
$ν(τ) ∝ G(τ) π(τ)$, we get

~~~
$$D_\textrm{KL}\big(ν \| π_{→θ}) = ∫_τ ν(τ) \log \frac{ν(τ)}{π_{→θ}(τ)} \d τ.$$

~~~
Therefore, the gradient is
$$\frac{∂ D_\textrm{KL}\big(ν \| π_{→θ}\big)}{∂ →θ}
 = - ∫_τ ν(τ) ∇_{→θ} \log π_{→θ}(τ) \d τ
 ∝ - ∫_τ π_{→θ}(τ) G(τ) ∇_{→θ} \log π_{→θ}(τ) \d τ.$$

~~~
For optimal policy $π_{→θ^*}$, we therefore get
$\frac{∂ D_\textrm{KL}(ν \| π_{→θ^*})}{∂ →θ^*} ∝ - ∫_τ π_{→θ^*}(τ) G(τ) ∇_{→θ^*} \log π_{→θ^*}(τ) \d τ$,
~~~
but the latter is zero because of the optimality of $π_{→θ^*}$ according to the
policy gradient theorem; therefore, $π_{→θ^*}$ is also the fixed point of $𝓟∘𝓘$.

---
# State-Action Formulation of OP-REINFORCE

We can formulate the operator view also employing the action-value function $q$
and the on-policy distribution $μ_π$; however, the policy improvement operator
needs to return not just a policy, but a joint distribution over the states
and actions.

~~~
The REINFORCE algorithm can be seen as performing one gradient step to minimize
the composition $𝓟∘𝓘$, where
$$(𝓘 π)(s, a) \stackrel{\tiny\textrm{def}}{∝} μ_π(s) q_π(s, a) π(a | s),$$
~~~
$$𝓟 ν ≝ \argmin_{→θ} 𝔼_{s∼ν(s)} \Big[D_\textrm{KL}\big(ν(⋅|s) \| π_{→θ}(⋅|s)\big)\Big].$$

---
# State-Action Formulation of OP-REINFORCE

For completeness, we can explicitly express the joint distribution
$(𝓘 π)(s, a)$ as a product of $(𝓘 π)(s) ⋅ (𝓘 π)(a|s)$, where
- the distribution over the states is
  $$(𝓘 π)(s) ≝ \frac{μ_π(s) v_π(s)}{∑_{s'} μ_π(s') v_π(s')} = \frac{μ_π(s) v_π(s)}{𝔼_{s'∼μ_π} [v_π(s')]},$$

~~~
- the conditional distribution over the actions is
  $$(𝓘 π)(a|s)
    ≝ \frac{q_π(s, a) π(a|s)}{∑_{a'} q_π(s, a') π(a'|s)}
    = \frac{q_π(s, a) π(a|s)}{𝔼_{a'∼π(s)}[q_π(s, a')]}
    = \frac{q_π(s, a) π(a|s)}{v_π(s)}.$$

---
class: dbend
# Higher Powers of the Returns

Instead of $ν(τ) ∝ G(τ) π(τ)$, we now for $k ≥ 1$ consider

$$(𝓘^k π)(τ) \stackrel{\tiny\textrm{def}}{∝} G(τ)^k π(τ).$$

~~~
However, it is not obvious if $π_{→θ^*}$ is still a fixed point of $𝓟∘𝓘^k$.
~~~
In fact, it is not:

![w=71%,h=center](op_reinforce_powers.svgz)

---
class: dbend
# Higher Powers of the Returns

Let reformulate the $𝓘^k$ to $𝓘^\frac{1}{α}$ for $α ≤ 1$:

$$(𝓘^\frac{1}{α} π)(τ) \stackrel{\tiny\textrm{def}}{∝} G(τ)^\frac{1}{α} π(τ).$$

~~~
We then define a projection operator $𝓟^α$ using $α$-divergence (also known as
Rényi divergence of order $α$) instead of the KL divergence:

$$\begin{aligned}
  𝓟^α ν & ≝ \argmin_{→θ} D^α\big(ν \| π_{→θ}\big),\\
  D^α(p\|q) & ≝ \frac{1}{1 - α} \log 𝔼_{x∼p} \big[(p(x)/q(x))^{α-1}\big] .\\
\end{aligned}$$

~~~
For $α=1$, $D^α$ is not defined, but its limit in $α → 1$ is $D_\mathrm{KL}$.

~~~
Proposition 8 of the OP-REINFORCE paper proves, that for $α ∈ (0,1)$, $π_{→θ^*}$
is the fixed point of $𝓟^α∘𝓘^\frac{1}{α}$.

---
class: dbend
# Higher Powers of the Returns

![w=73%,h=center](op_reinforce_powers.svgz)

~~~
Note that $𝓘^α$ in the limit $α → 0$ assigns probability of 1 to the greedy
action, so it becomes the _greedy policy improvement operator_.

---
section: Actor-Critic
# Actor-Critic

It is possible to combine the policy gradient methods and temporal difference
methods, creating a family of algorithms usually called the **actor-critic** methods.

~~~
The idea is straightforward – similarly to the REINFORCE with baseline, we train
the policy network together with the value network. However, instead of
estimating the episode return using the whole episode rewards, we use $n$-step
return TD estimate in both the policy gradient and the mean squared
value error $\overline{VE}$.

---
# Actor-Critic

![w=85%,h=center](actor_critic.svgz)

---
section: A3C
# Asynchronous Methods for Deep RL

The A3C was introduced in a 2016 paper from Volodymyr Mnih et al. (the same group as DQN)
[Asynchronous Methods for Deep Reinforcement Learning,
https://arxiv.org/abs/1602.01783](https://arxiv.org/abs/1602.01783).

~~~
The authors propose an asynchronous framework, where multiple workers share one
neural network, each training using either an off-line or on-line RL algorithm.

~~~
They compare 1-step Q-learning, 1-step Sarsa, $n$-step Q-learning and A3C
(an _asynchronous advantage actor-critic_ method). For A3C, they compare
a version with and without LSTM.

~~~
The authors also introduce _entropy regularization term_ $-β H(π(s; →θ))$ to the
loss to support exploration and discourage premature convergence (they use
$β=0.01$).

~~~
- The entropy regularization has since become the standard way of encouraging
  exploration with a policy network.

~~~
  The entropy regularization keeps a controllable level of surprise (i.e.,
  exploration) in the distribution.
~~~
  Compared to $ε$-greedy approach, the exploration actions are sampled
  proportionally to their expected utility, not randomly.

---
# Asynchronous Methods for Deep RL

![w=45%,h=center](asynchronous_q_learning.svgz)

---
# Asynchronous Methods for Deep RL

![w=69%,h=center](asynchronous_q_learning_nstep.svgz)

---
# Asynchronous Methods for Deep RL

![w=86.5%,h=center](a3c.svgz)

---
# Asynchronous Methods for Deep RL

All methods performed updates every 5 actions
($t_\textrm{max}=I_\textrm{AsyncUpdate}=5$), updating the target
network each $40\,000$ frames.

~~~
The Atari inputs were processed as in DQN, using also action repeat 4.

~~~
The network architecture is: 16 filters $8×8$ stride 4, 32
filters $4×4$ stride 2, followed by a fully connected layer with 256 units.
All hidden layers apply a ReLU nonlinearity. Values and/or action values
were then generated from the (same) last hidden layer.

~~~
The LSTM methods utilized a 256-unit LSTM cell after the dense hidden layer.

~~~
All experiments used a discount factor of $γ=0.99$ and used RMSProp with
momentum decay factor of $0.99$.

---
# Asynchronous Methods for Deep RL

![w=100%](a3c_performance.svgz)
![w=85%,mw=50%,h=center](a3c_performance_table.svgz)![w=50%](a3c_speedup.svgz)

---
# Asynchronous Methods for Deep RL

![w=92%,h=center](a3c_data_efficiency_episodes.svgz)

---
# Asynchronous Methods for Deep RL

![w=92%,h=center](a3c_data_efficiency_time.svgz)

---
# Asynchronous Methods for Deep RL

![w=100%,v=middle](a3c_learning_rates.svgz)
